{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries \n",
    "import cv2 \n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.applications.mobilenet_v2 import preprocess_input as mobile_preprocess \n",
    "import os\n",
    "from keras.preprocessing import image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_model_type = 'Mobilenet'\n",
    "mask_dim_dict = {'Normal': (150,150), 'Mobilenet': (224,224)}\n",
    "\n",
    "emotion_model_type = 'Normal'\n",
    "emotion_dim_dict = {'Normal': (48,48), 'Mobilenet': (224,224), 'VGG16': (224,224)}\n",
    "\n",
    "\n",
    "classifier = cv2.CascadeClassifier(f'ModelWeights/haarcascade_frontalface_default.xml')\n",
    "mask_model = load_model(f'ModelWeights/{mask_model_type}_Masks.h5')\n",
    "emotion_model = load_model(f'ModelWeights/{emotion_model_type}_Emotions.h5')\n",
    "\n",
    "\n",
    "emotion_dim = emotion_dim_dict[emotion_model_type]\n",
    "mask_dim = mask_dim_dict[mask_model_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_dim = (200,200)\n",
    "original_images= [] \n",
    "lime_images = [] \n",
    "for i in os.listdir('Tests/Mask'):\n",
    "    img = cv2.imread(f'Tests/Mask/{i}')\n",
    "    img = cv2.flip(img, 1,1)\n",
    "    stack_img = cv2.resize(img, stack_dim)\n",
    "    original_images.append(stack_img)\n",
    "    model_img = cv2.resize(img, mask_dim)\n",
    "    model_img = mobile_preprocess(model_img)\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    explanation = explainer.explain_instance(model_img, mask_model.predict, top_labels = 10, hide_color = 0, num_samples = 3000)\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only = False, num_features = 10, hide_rest = False)\n",
    "    lime_img = mark_boundaries(temp/2 + .5, mask)\n",
    "#     lime_img = cv2.resize(lime_img, stack_dim)\n",
    "    lime_images.append(lime_img)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('test', lime_images[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceMaskEmotionDetection",
   "language": "python",
   "name": "facemaskemotiondetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
