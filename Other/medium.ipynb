{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_value(path, dim, model_type): \n",
    "    '''This function will read an image and convert to a specified version and resize depending on which algorithm is being used.  If edge is specified as true, it will pass the img array to get_edged which returns a filtered version of the img'''\n",
    "    img = image.load_img(path, target_size = dim)\n",
    "    img = image.img_to_array(img)\n",
    "    if model_type == 'mobilenet': \n",
    "        img = preprocess_input(img)\n",
    "        return img\n",
    "    return img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_classes(class_type): \n",
    "    mask_paths = [f'../FaceMaskDataset/{class_type}/WithMask/{i}' for i in os.listdir(f'../FaceMaskDataset/{class_type}/WithMask')]\n",
    "    mask_labels = [1 for i in range(len(mask_paths))]\n",
    "    \n",
    "    nomask_paths = [f'../FaceMaskDataset/{class_type}/WithoutMask/{i}' for i in os.listdir(f'../FaceMaskDataset/{class_type}/WithoutMask')]\n",
    "    nomask_labels = [0 for i in range(len(nomask_paths))]\n",
    "    \n",
    "    labels = np.array(mask_labels + nomask_labels)\n",
    "    print(f'{class_type.upper()} Value Counts')\n",
    "    print(pd.Series(labels).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    paths = np.array(mask_paths + nomask_paths)\n",
    "    labels = to_categorical(labels)\n",
    "    paths, labels = sk_shuffle(paths, labels)\n",
    "    return paths, labels\n",
    "def get_mask_splits(dim, model_type): \n",
    "    \n",
    "    #Train Set\n",
    "    train_paths, train_labels = get_mask_classes('Train')\n",
    "    train_images = np.array([get_image_value(i, dim, model_type) for i in tqdm(train_paths, desc = 'Getting Train Images')])\n",
    "    train_dict = dict(images = train_images, labels = train_labels)\n",
    "\n",
    "    #Test Set\n",
    "    test_paths, test_labels = get_mask_classes('Test')\n",
    "    test_images = np.array([get_image_value(i, dim, model_type) for i in tqdm(test_paths, desc = 'Getting Test Images')])\n",
    "    test_images, test_labels = sk_shuffle(test_images, test_labels)\n",
    "    \n",
    "    #Validation Set\n",
    "    val_paths, val_labels = get_mask_classes('Validation')\n",
    "    val_images = np.array([get_image_value(i, dim, model_type) for i in tqdm(val_paths, desc = 'Getting Validation Images')])\n",
    "    val_images, val_labels = sk_shuffle(val_images, val_labels)\n",
    "    \n",
    "        \n",
    "    return train_images, test_images, train_labels, test_labels, val_images, val_labels\n",
    "    \n",
    "\n",
    "dim = (224,224)\n",
    "x_train, x_test, y_train, y_test, x_val, y_val = get_mask_splits(\n",
    "    dim, model_type = 'mobilenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import cv2\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenet(dim):\n",
    "    model = Sequential()\n",
    "    optimizer = Adam(lr = .0005)\n",
    "    baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "        input_tensor=Input(shape=dim))\n",
    "    \n",
    "    model.add(baseModel)\n",
    "    model.add(AveragePooling2D(pool_size=(7, 7)))\n",
    "    model.add(Flatten(name=\"flatten\"))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(2, activation=\"sigmoid\", name = 'Output'))\n",
    "\n",
    "    \n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience=5, min_delta = .00075)\n",
    "model_checkpoint = ModelCheckpoint(f'ModelWeights/Mobilenet_Masks.h5', verbose = 1, save_best_only=True,\n",
    "                                  monitor = 'val_loss')\n",
    "lr_plat = ReduceLROnPlateau(patience = 5, mode = 'min')\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "    \n",
    "\n",
    "dim = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "mobilenet = get_mobilenet(dim =dim)\n",
    "\n",
    "    \n",
    "augmentation =ImageDataGenerator(rotation_range = 20, width_shift_range = .2, height_shift_range = .2, \n",
    "                                                       horizontal_flip = True, shear_range = .15, \n",
    "                                 fill_mode = 'nearest', zoom_range = .15)\n",
    "augmentation.fit(x_train)\n",
    "mobilenet_history = mobilenet.fit_generator(augmentation.flow(x_train, y_train, batch_size = batch_size),\n",
    "            epochs = epochs, \n",
    "     callbacks = [early_stopping, model_checkpoint, lr_plat], validation_data = (x_test, y_test), verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_classes(class_type, max_values): \n",
    "    angry_paths = [f'../EmotionDataset/{class_type}/angry/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/angry')][:max_values]\n",
    "    angry_labels = [0 for i in range(len(angry_paths))]\n",
    "    \n",
    "    happy_paths = [f'../EmotionDataset/{class_type}/happy/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/happy')][:max_values] \n",
    "    happy_labels = [1 for i in range(len(happy_paths))]\n",
    "    \n",
    "    neutral_paths = [f'../EmotionDataset/{class_type}/neutral/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/neutral')][:max_values] \n",
    "    \n",
    "    neutral_labels = [2 for i in range(len(neutral_paths))]\n",
    "    \n",
    "    labels = np.array(angry_labels + happy_labels + neutral_labels)\n",
    "    \n",
    "    print(f'{class_type.upper()} Value Count')\n",
    "    print(pd.Series(labels).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    labels = to_categorical(labels)\n",
    "    paths = np.array(angry_paths + happy_paths + neutral_paths)\n",
    "    paths, labels = sk_shuffle(paths, labels)\n",
    "    return paths, labels\n",
    "\n",
    "#0: angry  1: happy  2: neutral  \n",
    "def get_emotion_splits(dim, model_type = 'mobilenet', max_values = 6000): \n",
    "        \n",
    "    train_paths, train_labels = get_emotion_classes('train', max_values)\n",
    "    test_paths, test_labels = get_emotion_classes('test', max_values)\n",
    "    \n",
    "    train_images = np.array([get_image_value(i, dim, model_type) for i in tqdm(train_paths, desc = 'Getting Train Images')])\n",
    "    test_images = np.array([get_image_value(i, dim, model_type) for i in tqdm(test_paths, desc = 'Getting Test Images')])\n",
    "    \n",
    "    return train_images, test_images, train_labels, test_labels \n",
    "\n",
    "x_train, x_test, y_train, y_test = get_emotion_splits(dim = (48,48), model_type = 'normal', max_values = 4000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model(dim): \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape = dim))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(.35))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.35))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation = 'softmax', name = 'Output'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience=5, min_delta = .00075)\n",
    "model_checkpoint = ModelCheckpoint(f'ModelWeights/Normal_Emotions.h5', verbose = 1, save_best_only=True,\n",
    "                                  monitor = 'val_loss')\n",
    "lr_plat = ReduceLROnPlateau(patience = 5, mode = 'min')\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "augment = False  \n",
    "\n",
    "dim = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "cnn = get_conv_model(dim =dim)\n",
    "\n",
    "cnn_history = cnn.fit(x_train, y_train, batch_size = batch_size,\n",
    "          epochs = epochs, \n",
    "   callbacks = [early_stopping, model_checkpoint, lr_plat], validation_data = (x_test, y_test), verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_ellesmere.0\"\n"
     ]
    }
   ],
   "source": [
    "#live video\n",
    "\n",
    "classifier = cv2.CascadeClassifier(f'ModelWeights/haarcascade_frontalface_default.xml')\n",
    "mask_model = load_model(f'ModelWeights/Mobilenet_Masks.h5')\n",
    "emotion_model = load_model(f'ModelWeights/Normal_Emotions.h5')\n",
    "\n",
    "emotion_dim = (48,48)\n",
    "mask_dim = (224,224)\n",
    "\n",
    "mask_dict = {0: 'No Mask', 1: 'Mask'}\n",
    "mask_dict_color = {0: (0,0,255), 1: (0,255,0)} #colors for bounding boxes\n",
    "emotion_dict = {0: 'angry', 1: 'happy', 2: 'neutral'}\n",
    "\n",
    "vid_frames = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1,1)\n",
    "    clone = frame.copy()\n",
    "    bboxes = classifier.detectMultiScale(clone)\n",
    "    for i in bboxes: \n",
    "        x, y, width, height = i[0], i[1], i[2], i[3]\n",
    "        x2, y2 = x+ width, y+height\n",
    "        mask_roi = clone[y:y2, x:x2]\n",
    "        emotion_roi = mask_roi.copy()\n",
    "        emotion_roi = cv2.resize(emotion_roi, emotion_dim, interpolation = cv2.INTER_CUBIC)\n",
    "        mask_roi = cv2.resize(mask_roi, mask_dim, interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        #preprocess mask_input\n",
    "        mask_roi= preprocess_input(mask_roi)\n",
    "    \n",
    "        #preprocess emotion input\n",
    "        emotion_roi = emotion_roi/255\n",
    "        \n",
    "        #resize emotion and mask to feed into nn   \n",
    "        mask_roi = mask_roi.reshape(1, mask_roi.shape[0], mask_roi.shape[1], mask_roi.shape[2])\n",
    "        emotion_roi = emotion_roi.reshape(1, emotion_roi.shape[0], emotion_roi.shape[1], emotion_roi.shape[2])\n",
    "        #mask predictions\n",
    "        mask_predict = mask_model.predict(mask_roi)[0]\n",
    "        mask_idx = np.argmax(mask_predict)\n",
    "        mask_conf = f'{round(np.max(mask_predict)*100)}%'\n",
    "        mask_cat = mask_dict[mask_idx]\n",
    "        mask_color = mask_dict_color[mask_idx]\n",
    "        if mask_idx == 0: #if there is no mask detected --> move onto emotions\n",
    "            #emotion predictions\n",
    "            emotion_predict = emotion_model.predict(emotion_roi)[0]\n",
    "            emotion_idx = np.argmax(emotion_predict)\n",
    "            emotion_cat = emotion_dict[emotion_idx]\n",
    "            emotion_conf = f'{round(np.max(emotion_predict)*100)}%'\n",
    "            cv2.putText(clone, f'{mask_cat}: {mask_conf} || {emotion_cat}: {emotion_conf}', (x, y+15), cv2.FONT_HERSHEY_SIMPLEX, .5, mask_color, 2)\n",
    "            cv2.rectangle(clone, (x,y), (x2,y2), mask_color, 1)\n",
    "            continue\n",
    "        cv2.putText(clone, f'{mask_cat}: {mask_conf}', (x, y+15), cv2.FONT_HERSHEY_SIMPLEX, .5, mask_color, 2)\n",
    "        cv2.rectangle(clone, (x,y), (x2,y2), mask_color, 1)\n",
    "               \n",
    "    cv2.imshow('LIVE', clone)\n",
    "    vid_frames.append(clone)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF ==ord('q'): \n",
    "        break \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceMaskEmotionDetection",
   "language": "python",
   "name": "facemaskemotiondetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
