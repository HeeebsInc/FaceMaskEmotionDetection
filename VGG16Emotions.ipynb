{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras import regularizers\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PyFunctions import Functions as func\n",
    "from PyFunctions import Viz\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Value Count\n",
      "1    6000\n",
      "2    4965\n",
      "3    4830\n",
      "0    3995\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "TEST Value Count\n",
      "1    1774\n",
      "3    1247\n",
      "2    1233\n",
      "0     958\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "(19790, 224, 224, 3) (5212, 224, 224, 3)\n",
      "(19790, 4) (5212, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = func.get_emotion_splits(dim = (224,224), model_type = 'vgg16') \n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg16(dim):\n",
    "    model = Sequential()\n",
    "    baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "        input_tensor=Input(shape=dim), classes = 4)\n",
    "    \n",
    "    # loop over all layers in the base model and freeze them so they will\n",
    "    # *not* be updated during the first training process\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.add(baseModel)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "#     model.add(Flatten(name=\"flatten\"))\n",
    "    model.add(Dense(1024, activation=\"relu\"))\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation=\"softmax\", name = 'Output'))\n",
    " \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "310/310 [==============================] - 584s 2s/step - loss: 1.2218 - acc: 0.4602 - val_loss: 1.2138 - val_acc: 0.4605\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21382, saving model to ModelWeights/VGG16_Emotions.h5\n",
      "Epoch 2/2000\n",
      "310/310 [==============================] - 582s 2s/step - loss: 1.2142 - acc: 0.4600 - val_loss: 1.2232 - val_acc: 0.4526\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21382\n",
      "Epoch 3/2000\n",
      "310/310 [==============================] - 584s 2s/step - loss: 1.1984 - acc: 0.4742 - val_loss: 1.2059 - val_acc: 0.4705\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.21382 to 1.20587, saving model to ModelWeights/VGG16_Emotions.h5\n",
      "Epoch 4/2000\n",
      "310/310 [==============================] - 584s 2s/step - loss: 1.1929 - acc: 0.4770 - val_loss: 1.2150 - val_acc: 0.4637\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.20587\n",
      "Epoch 5/2000\n",
      "310/310 [==============================] - 581s 2s/step - loss: 1.1867 - acc: 0.4788 - val_loss: 1.2104 - val_acc: 0.4547\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.20587\n",
      "Epoch 6/2000\n",
      "310/310 [==============================] - 581s 2s/step - loss: 1.1839 - acc: 0.4740 - val_loss: 1.1869 - val_acc: 0.4731\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.20587 to 1.18688, saving model to ModelWeights/VGG16_Emotions.h5\n",
      "Epoch 7/2000\n",
      "310/310 [==============================] - 581s 2s/step - loss: 1.1784 - acc: 0.4818 - val_loss: 1.1796 - val_acc: 0.4843\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.18688 to 1.17963, saving model to ModelWeights/VGG16_Emotions.h5\n",
      "Epoch 8/2000\n",
      "310/310 [==============================] - 580s 2s/step - loss: 1.1745 - acc: 0.4878 - val_loss: 1.1994 - val_acc: 0.4682\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.17963\n",
      "Epoch 9/2000\n",
      "310/310 [==============================] - 580s 2s/step - loss: 1.1702 - acc: 0.4882 - val_loss: 1.1860 - val_acc: 0.4772\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17963\n",
      "Epoch 10/2000\n",
      "310/310 [==============================] - 580s 2s/step - loss: 1.1644 - acc: 0.4874 - val_loss: 1.1569 - val_acc: 0.4965\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.17963 to 1.15695, saving model to ModelWeights/VGG16_Emotions.h5\n",
      "Epoch 11/2000\n",
      "310/310 [==============================] - 585s 2s/step - loss: 1.1611 - acc: 0.4923 - val_loss: 1.1981 - val_acc: 0.4766\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.15695\n",
      "Epoch 12/2000\n",
      "310/310 [==============================] - 583s 2s/step - loss: 1.1503 - acc: 0.4970 - val_loss: 1.1841 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.15695\n",
      "Epoch 13/2000\n",
      "310/310 [==============================] - 582s 2s/step - loss: 1.1516 - acc: 0.4967 - val_loss: 1.1900 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.15695\n",
      "Epoch 14/2000\n",
      "310/310 [==============================] - 580s 2s/step - loss: 1.1509 - acc: 0.4968 - val_loss: 1.1758 - val_acc: 0.4854\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.15695\n",
      "Epoch 15/2000\n",
      "310/310 [==============================] - 581s 2s/step - loss: 1.1427 - acc: 0.5000 - val_loss: 1.2149 - val_acc: 0.4649\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.15695\n",
      "Epoch 16/2000\n",
      "310/310 [==============================] - 584s 2s/step - loss: 1.1179 - acc: 0.5166 - val_loss: 1.1791 - val_acc: 0.4814\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.15695\n",
      "Epoch 17/2000\n",
      "310/310 [==============================] - 582s 2s/step - loss: 1.1126 - acc: 0.5229 - val_loss: 1.1814 - val_acc: 0.4837\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.15695\n",
      "Epoch 18/2000\n",
      "310/310 [==============================] - 583s 2s/step - loss: 1.1114 - acc: 0.5210 - val_loss: 1.1800 - val_acc: 0.4850\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.15695\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience=8, min_delta = .00075)\n",
    "model_checkpoint = ModelCheckpoint(f'ModelWeights/VGG16_Emotions.h5', verbose = 1, save_best_only=True,\n",
    "                                  monitor = 'val_loss')\n",
    "lr_plat = ReduceLROnPlateau(patience = 5, mode = 'min')\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "    \n",
    "\n",
    "dim = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "vgg16 = get_vgg16(dim =dim)\n",
    "vgg16 = load_model('ModelWeights/VGG16_Emotions.h5')\n",
    "    \n",
    "augmentation =ImageDataGenerator(rotation_range = 20, width_shift_range = .2, height_shift_range = .2, \n",
    "                                                       horizontal_flip = True, shear_range = .15, \n",
    "                                 fill_mode = 'nearest', zoom_range = .15)\n",
    "augmentation.fit(x_train)\n",
    "vgg16_history = vgg16.fit_generator(augmentation.flow(x_train, y_train, batch_size = batch_size),\n",
    "            epochs = epochs, \n",
    "     callbacks = [early_stopping, model_checkpoint, lr_plat], validation_data = (x_test, y_test), verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceMaskEmotionDetection",
   "language": "python",
   "name": "facemaskemotiondetection"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
