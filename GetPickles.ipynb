{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_value(path, dim, bw, model_type): \n",
    "    '''This function will read an image and convert to a specified version and resize depending on which algorithm is being used.  If edge is specified as true, it will pass the img array to get_edged which returns a filtered version of the img'''\n",
    "    img = image.load_img(path, target_size = dim)\n",
    "    img = image.img_to_array(img)\n",
    "    if bw == True: \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img.reshape(img.shape[0], img.shape[1],1)\n",
    "    if model_type == 'mobilenet': \n",
    "        img = preprocess_input(img)\n",
    "        return img\n",
    "    return img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Getting Emotion Train Images:   0%|                                                                                                                             | 0/28709 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Value Count\n",
      "3    7215\n",
      "4    4965\n",
      "5    4830\n",
      "2    4097\n",
      "0    3995\n",
      "6    3171\n",
      "1     436\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "TEST Value Count\n",
      "3    1774\n",
      "5    1247\n",
      "4    1233\n",
      "2    1024\n",
      "0     958\n",
      "6     831\n",
      "1     111\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Emotion Train Images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28709/28709 [02:55<00:00, 163.30it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 16.1 GiB for an array with shape (28709, 224, 224, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-21be6c5a7870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mtts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_emotion_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpick_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Mobilenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-21be6c5a7870>\u001b[0m in \u001b[0;36mget_emotion_splits\u001b[1;34m(dim, pick_name, model_type, bw)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mtest_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_emotion_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_image_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Getting Emotion Train Images'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mtest_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_image_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Getting Emotion Test Images'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 16.1 GiB for an array with shape (28709, 224, 224, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "def get_emotion_classes(class_type): \n",
    "        angry_paths = [f'../EmotionDataset/{class_type}/angry/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/angry')]\n",
    "    angry_labels = [0 for i in range(len(angry_paths))]\n",
    "    \n",
    "    happy_paths = [f'../EmotionDataset/{class_type}/happy/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/happy')] \n",
    "    happy_labels = [1 for i in range(len(happy_paths))]\n",
    "    \n",
    "    neutral_paths = [f'../EmotionDataset/{class_type}/neutral/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/neutral')] \n",
    "    neutral_labels = [2 for i in range(len(neutral_paths))]\n",
    "    \n",
    "    sad_paths = [f'../EmotionDataset/{class_type}/sad/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/sad')] \n",
    "    sad_labels = [3 for i in range(len(sad_paths))]\n",
    "    \n",
    "    disgust_paths = [f'../EmotionDataset/{class_type}/disgust/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/disgust')]\n",
    "    disgust_labels = [4 for i in range(len(disgust_paths))]\n",
    "    \n",
    "    fear_paths = [f'../EmotionDataset/{class_type}/fear/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/fear')]\n",
    "    fear_labels = [5 for i in range(len(fear_paths))]\n",
    "    \n",
    "    surprise_paths = [f'../EmotionDataset/{class_type}/surprise/{i}' for i in os.listdir(f'../EmotionDataset/{class_type}/surprise')] \n",
    "    surprise_labels = [6 for i in range(len(surprise_paths))]\n",
    "    \n",
    "    labels = np.array(angry_labels + disgust_labels + fear_labels + happy_labels + \\\n",
    "                        neutral_labels + sad_labels + surprise_labels)\n",
    "    \n",
    "    print(f'{class_type.upper()} Value Count')\n",
    "    print(pd.Series(labels).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    labels = to_categorical(labels)\n",
    "    paths = np.array(angry_paths + disgust_paths + fear_paths + happy_paths + neutral_paths + \\\n",
    "                          sad_paths + surprise_paths)\n",
    "    paths, labels = sk_shuffle(paths, labels)\n",
    "    return paths, labels\n",
    "#0: angry  || 1: happy || 2: neutral || 3: sad || 4: disgust || 5: fear ||  6: surprise\n",
    "\n",
    "def get_emotion_splits(dim, pick_name, model_type = 'Mobilenet', bw = False): \n",
    "    \n",
    "    #Train\n",
    "    \n",
    "    train_paths, train_labels = get_emotion_classes('train')\n",
    "    test_paths, test_labels = get_emotion_classes('test')\n",
    "    \n",
    "    train_images = np.array([get_image_value(i, dim, bw, model_type) for i in tqdm(train_paths, desc = 'Getting Emotion Train Images')])\n",
    "    test_images = np.array([get_image_value(i, dim, bw, model_type) for i in tqdm(test_paths, desc = 'Getting Emotion Test Images')])\n",
    "    \n",
    "    if model_type == 'Mobilenet' and bw == True: \n",
    "        train_images = np.stack((train_images,)*3, axis =-1)\n",
    "        test_images = np.stack((test_images,)*3, axis = -1)\n",
    "    \n",
    "    tts = (train_images, test_images, train_labels, test_labels)\n",
    "    \n",
    "    pickle.dump(tts, open(f'../Pickles/TTSEmotion_{pick_name}.p', 'wb'), protocol = 4)\n",
    "    print('Finished Pickling Emotions')\n",
    "    \n",
    "    return tts\n",
    "dim = (224,224)\n",
    "tts = get_emotion_splits(dim, pick_name = 'Mobilenet', bw = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Train Images:   0%|                                                                                                                            | 10/10000 [00:00<01:41, 98.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Value Counts\n",
      "1    5000\n",
      "0    5000\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Train Images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:07<00:00, 147.91it/s]\n",
      "Getting Test Images:   1%|█▍                                                                                                                            | 11/992 [00:00<00:09, 102.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Value Counts\n",
      "0    509\n",
      "1    483\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Test Images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 992/992 [00:04<00:00, 228.44it/s]\n",
      "Getting Validation Images:   2%|██▋                                                                                                                     | 18/800 [00:00<00:04, 179.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION Value Counts\n",
      "1    400\n",
      "0    400\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Validation Images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:01<00:00, 430.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling The Data\n"
     ]
    }
   ],
   "source": [
    "def get_mask_classes(class_type): \n",
    "    mask_paths = [f'../FaceMaskDataset/{class_type}/WithMask/{i}' for i in os.listdir(f'../FaceMaskDataset/{class_type}/WithMask')]\n",
    "    mask_labels = [1 for i in range(len(mask_paths))]\n",
    "    \n",
    "    nomask_paths = [f'../FaceMaskDataset/{class_type}/WithoutMask/{i}' for i in os.listdir(f'../FaceMaskDataset/{class_type}/WithoutMask')]\n",
    "    nomask_labels = [0 for i in range(len(nomask_paths))]\n",
    "    \n",
    "    labels = np.array(mask_labels + nomask_labels)\n",
    "    print(f'{class_type.upper()} Value Counts')\n",
    "    print(pd.Series(labels).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    paths = np.array(mask_paths + nomask_paths)\n",
    "    labels = to_categorical(labels)\n",
    "    paths, labels = sk_shuffle(paths, labels)\n",
    "    return paths, labels\n",
    "def get_mask_splits(dim, pick_name, model_type = 'Mobilenet', bw = False): \n",
    "    \n",
    "    #Train Set\n",
    "    train_paths, train_labels = get_mask_classes('Train')\n",
    "    train_images = np.array([get_image_value(i, dim, bw, model_type) for i in tqdm(train_paths, desc = 'Getting Train Images')])\n",
    "    train_dict = dict(images = train_images, labels = train_labels)\n",
    "\n",
    "    #Test Set\n",
    "    test_paths, test_labels = get_mask_classes('Test')\n",
    "    test_images = np.array([get_image_value(i, dim, bw, model_type) for i in tqdm(test_paths, desc = 'Getting Test Images')])\n",
    "    test_images, test_labels = sk_shuffle(test_images, test_labels)\n",
    "    \n",
    "    #Validation Set\n",
    "    val_paths, val_labels = get_mask_classes('Validation')\n",
    "    val_images = np.array([get_image_value(i, dim, bw, model_type) for i in tqdm(val_paths, desc = 'Getting Validation Images')])\n",
    "    val_images, val_labels = sk_shuffle(val_images, val_labels)\n",
    "    \n",
    "    tts = train_images, test_images, train_labels, test_labels, val_images, val_labels\n",
    "    \n",
    "    print('Pickling The Data')\n",
    "    pickle.dump(tts, open(f'../Pickles/TTSMask_{pick_name}.p', 'wb'), protocol = 4)\n",
    "    \n",
    "    return tts\n",
    "    \n",
    "\n",
    "dim = (224,224)\n",
    "return_dict = get_mask_splits(dim, pick_name = 'MobilenetTest', bw = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GITHUB\n",
    "def get_image_value(path, dim, bw, model_type = 'Normal'): \n",
    "    '''This function will read an image and convert to a specified version and resize depending on which algorithm is being used.  If edge is specified as true, it will pass the img array to get_edged which returns a filtered version of the img'''\n",
    "    img = image.load_img(path, target_size = dim)\n",
    "    img = image.img_to_array(img)\n",
    "    if bw == True: \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img.reshape(img.shape[0], img.shape[1],1)\n",
    "    if model_type == 'mobilenet': \n",
    "        img = preprocess_input(img)\n",
    "        return img\n",
    "    return img/255\n",
    "\n",
    "\n",
    "def get_splits(dim, pick_name, model_type = 'Normal', bw = False): \n",
    "    \n",
    "    mask_paths = [f'../GithubData/with_mask/{i}' for i in os.listdir('../GithubData/with_mask')]\n",
    "    mask_labels = [1 for i in range(len(mask_paths))]\n",
    "    mask_images = [get_image_value(i, dim, bw, model_type) for i in tqdm(mask_paths, desc = 'Getting Images With Mask')]\n",
    "    \n",
    "    nomask_paths = [f'../GithubData/without_mask/{i}' for i in os.listdir('../GithubData/without_mask')]\n",
    "    nomask_labels = [0 for i in range(len(nomask_paths))]\n",
    "    nomask_images = [get_image_value(i, dim, bw, model_type) for i in tqdm(nomask_paths, desc = 'Getting Images With No Mask')]\n",
    "\n",
    "    image_list = np.array(nomask_images + mask_images)\n",
    "    labels = np.array(nomask_labels + mask_labels)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(image_list, labels, stratify = labels, random_state = 10, train_size = .8)\n",
    "    \n",
    "    print('Train Value Counts')\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Test Value Counts')\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    \n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)    \n",
    "    \n",
    "    tts = (x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    print('Finished Pickling TTS')\n",
    "    pickle.dump(tts, open(f'../Pickles/TTS_{pick_name}.p', 'wb'), protocol = 4)\n",
    "    \n",
    "    return tts\n",
    "    \n",
    "\n",
    "dim = (224,224)\n",
    "return_dict = get_splits(dim, pick_name = 'GithubMobile', model_type = 'mobilenet', bw = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceMaskEmotionDetection",
   "language": "python",
   "name": "facemaskemotiondetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
